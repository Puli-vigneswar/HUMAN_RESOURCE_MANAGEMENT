{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import streamlit\n",
    "\n",
    "# Set the path to the directory where your CSV files are stored\n",
    "path = 'D:/HUMAN_RESOURCE_MANAGMENT'  # replace with your CSV files' directory path\n",
    "all_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.csv')]\n",
    "\n",
    "# List to hold dataframes\n",
    "df_list = []\n",
    "\n",
    "# Loop through all CSV files and append to the list\n",
    "for filename in all_files:\n",
    "    \n",
    "    df = pd.read_csv(filename, index_col=None, header=0, encoding='ISO-8859-1')\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "data= pd.concat(df_list, axis=0, ignore_index=True)\n",
    "\n",
    "# Now you have a new dataframe 'combined_df' with all your CSV data\n",
    "\n",
    "data\n",
    "# This will display the DataFrame with some default styling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={\n",
    "    'NIC Name': 'NIC_NAME',\n",
    "    \"India/States\": \"state_ut\",\n",
    "    'State Code': 'State_Code',\n",
    "    'District Code': 'District_Code'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"NIC_NAME\"]==\"Incomplete description/ Wrongly Classifed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[~data['NIC_NAME'].isin([\"Blank\",\"Incomplete description/ Wrongly Classifed\",\"incomplete\",\"BLANK\",\"Blank etc.\"])]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['state_ut']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total words state wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=data[data[\"NIC_NAME\"].str.lower()==\"total\"]\n",
    "df = h[h['state_ut'].str.startswith('STATE')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "state totals dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasets exported here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data.state_ut.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.state_ut.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"state_ut\"]=data[\"state_ut\"].str.lower()\n",
    "data[\"NIC_NAME\"]=data[\"NIC_NAME\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "state only data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_mask = data['state_ut'].str.startswith('state')\n",
    "state_df = data[state_mask]\n",
    "state_df\n",
    "district= data['state_ut'].str.startswith('district')\n",
    "dist_df=data[district]\n",
    "dist_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hg=dist_df[dist_df[\"NIC_NAME\"]==\"total\"]\n",
    "hg.drop_duplicates(subset=['District_Code', 'state_ut'], keep='last')\n",
    "hg.to_csv('D:/total_NIC_dist.csv', index=False)\n",
    "\n",
    "\n",
    "hg=state_df[state_df[\"NIC_NAME\"]==\"total\"]\n",
    "hg.drop_duplicates(subset=['State_Code', 'state_ut'], keep='last')\n",
    "hg.to_csv('D:/total_NIC_state.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdup=state_df.state_ut.value_counts()\n",
    "stdup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distdup=dist_df.state_ut.value_counts().duplicated\n",
    "distdup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new= data.drop_duplicates(subset=['State_Code', 'District_Code', 'state_ut', 'Division',\n",
    "        'Main Workers - Total -  Persons',\n",
    "       'Main Workers - Total - Males', 'Main Workers - Total - Females',\n",
    "       'Main Workers - Rural -  Persons', 'Main Workers - Rural - Males',\n",
    "       'Main Workers - Rural - Females', 'Main Workers - Urban -  Persons',\n",
    "       'Main Workers - Urban - Males', 'Main Workers - Urban - Females',\n",
    "       'Marginal Workers - Total -  Persons',\n",
    "       'Marginal Workers - Total - Males',\n",
    "       'Marginal Workers - Total - Females',\n",
    "       'Marginal Workers - Rural -  Persons',\n",
    "       'Marginal Workers - Rural - Males',\n",
    "       'Marginal Workers - Rural - Females',\n",
    "       'Marginal Workers - Urban -  Persons',\n",
    "       'Marginal Workers - Urban - Males',\n",
    "       'Marginal Workers - Urban - Females'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Tokenize the text in the 'text_column'\n",
    "new['tokenized_NIC'] = new['NIC_NAME'].apply(lambda text: nltk.word_tokenize(text))\n",
    "def stemmed(tokens):\n",
    "    post=PorterStemmer()\n",
    "    txt=\" \".join([post.stem(word) for word in tokens])\n",
    "    return txt\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    text=\" \".join([lemmatizer.lemmatize(word,pos='v') for word in tokens])\n",
    "    return text\n",
    "\n",
    "new['removed_stops'] = new['tokenized_NIC'].apply(remove_stopwords)\n",
    "new['lemmatized'] = new['removed_stops'].apply(lemmatize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "statewise entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries=pd.DataFrame(new.State_Code.value_counts())\n",
    "#top=entries.max()\n",
    "#low=entries.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=pd.DataFrame(new.District_Code.value_counts())\n",
    "d.sort_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nic=pd.DataFrame(new.NIC_NAME.value_counts())\n",
    "hj=nic.sort_index()\n",
    "hj[25:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hj[200:225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.District_Code.isna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "ax=sns.displot(new[\"State_Code\"])\n",
    "ax.tick_params(axis='x',rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"District_Code\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Our sentences we like to encode\n",
    "sentences = [new['lemmatized']\n",
    "]\n",
    "\n",
    "# Sentences are encoded by calling model.encode()\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "sentence_embeddings'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total word in nic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvals=new[new['NIC_NAME']==\"total\"]\n",
    "tl=totalvals.District_Code.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new['lemmatized'].count()\n",
    "new=new[~new['lemmatized'].isin([\"total\"])]\n",
    "new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = new['lemmatized'].unique().tolist()\n",
    "len(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load a pre-trained Sentence-Transformer model (e.g., 'paraphrase-MiniLM-L6-v2')\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "unique_values\n",
    "embeddings = model.encode(unique_values)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 4 # Example number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(embeddings)\n",
    "\n",
    "\n",
    "df5 = pd.DataFrame({'lemmatized': unique_values})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df5['embeddings'] = df5['lemmatized'].apply(lambda x: model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new11=new.join(df5.set_index('lemmatized'), on='lemmatized', validate='m:1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply K-means clustering\n",
    "num_clusters = 4 # Specify the number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "new11['cluster'] = kmeans.fit_predict(list(new11['embeddings']))\n",
    "\n",
    "# Print the clustered sentences\n",
    "'''for cluster_id in range(num_clusters):\n",
    "    cluster_sentences = new1[new1['cluster'] == cluster_id]['lemmatized']\n",
    "    print(f\"Cluster {cluster_id + 1} ({len(cluster_sentences)} sentences):\")\n",
    "    print(cluster_sentences.tolist())\n",
    "    \n",
    "\n",
    "# Optional: You can explore the cluster centroids (cluster centers)\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "print(\"Cluster Centers (Embeddings):\")\n",
    "print(cluster_centers)'''\n",
    "\n",
    "# Optional: Assign meaningful labels to clusters based on content\n",
    "# For example, you can manually inspect the sentences in each cluster\n",
    "# and label them accordingly.\n",
    "\n",
    "# Note: Adjust the model, data, and parameters according to your specific use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new11.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf1 = new11[new11['cluster'] == 0]\n",
    "newdf1.to_csv('D:/clusteris0.csv', index=False)  # Specify the desired file name\n",
    "newdf2 = new11[new11['cluster'] == 1]\n",
    "newdf2.to_csv('D:/clusteris1.csv', index=False)\n",
    "newdf3 = new11[new11['cluster'] == 2]\n",
    "newdf3.to_csv('D:/clusteris2.csv', index=False)\n",
    "newdf4 = new11[new11['cluster'] == 3]\n",
    "newdf4.to_csv('D:/clusteris4.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
